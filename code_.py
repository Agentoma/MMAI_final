# -*- coding: utf-8 -*-
"""code .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PGoYahtOQRYCiODcrPDBbl1SJtefi7P3

**PART 1: Dijkstra Algotrithm to find the shortest distance**
"""

# Re-defining the graph explicitly before running the manual Dijkstra algorithm
distance_map = {
    'Origin': {'Vaughan': 20, 'Richmond Hill': 30, 'Markham': 25},
    'Vaughan': {'Richmond Hill': 5, 'North York': 35},
    'Richmond Hill': {'Markham': 25, 'North York': 20},
    'Markham': {'North York': 25, 'Destination': 50},
    'North York': {'Destination': 30},
    'Destination': {}
}

def dijkstra(distance_map, start):
    distances = {node: float('inf') for node in distance_map}
    previous = {node: None for node in distance_map}
    distances[start] = 0
    visited = set()

    while len(visited) < len(distance_map):
        current = min(
            (node for node in distances if node not in visited),
            key=lambda x: distances[x],
            default=None
        )
        if current is None:
            break

        visited.add(current)

        for neighbor, weight in distance_map[current].items():
            new_distance = distances[current] + weight
            if new_distance < distances[neighbor]:
                distances[neighbor] = new_distance
                previous[neighbor] = current

    return distances, previous

def reconstruct_path(previous, start, end):
    path = []
    current = end
    while current != start:
        path.insert(0, current)
        current = previous[current]
        if current is None:
            return None
    path.insert(0, start)
    return path

# Run the algorithm
distances, previous = dijkstra(distance_map, 'Origin')
shortest_distance = distances['Destination']
shortest_path = reconstruct_path(previous, 'Origin', 'Destination')

# Print results
print(f"Shortest distance to Destination: {shortest_distance}")
print(f"Shortest path: {' -> '.join(shortest_path)}")

"""**PART 2:**

**1. Value Iteration Algorithm**
"""

# Define the states and actions
states = [1, 2, 3]
actions = [0, 1]  # 0 = no ad, 1 = advertise

# Rewards for each state
base_rewards = {
    1: 500,
    2: 1000,
    3: 3000
}

# Advertisement cost
ad_cost = 100

# Transition probabilities
P = {
    0: {  # No advertisement
        1: [0.4, 0.6, 0.0],
        2: [0.3, 0.4, 0.3],
        3: [0.0, 0.6, 0.4]
    },
    1: {  # Advertisement
        1: [0.1, 0.9, 0.0],
        2: [0.1, 0.6, 0.3],
        3: [0.0, 0.35, 0.65]
    }
}


gamma = 0.9     # Discount factor
epsilon = 1e-6  # Convergence threshold

def value_iteration(states, actions, P, base_rewards, ad_cost, gamma, epsilon):
    V = {j: 0 for j in states}         # Initialize V(j) = 0
    policy = {j: 0 for j in states}    # Initial policy π(j) = 0 (no ad)

    while True:
        delta = 0
        new_V = {}

        # Policy evaluation and improvement
        for j in states:
            action_values = []

            for a in actions:
                R = base_rewards[j] - (ad_cost if a == 1 else 0)
                sum_pvj = sum(P[a][j][j_prime - 1] * V[j_prime] for j_prime in states)
                value = R + gamma * sum_pvj
                action_values.append(value)

            best_value = max(action_values)
            best_action = actions[action_values.index(best_value)]

            new_V[j] = best_value
            policy[j] = best_action
            delta = max(delta, abs(V[j] - best_value))

        V = new_V

        # Convergence check
        if delta < epsilon:
            break

    return policy, V

    # Run value iteration
optimal_policy_vi, value_function_vi = value_iteration(states, actions, P, base_rewards, ad_cost, gamma, epsilon)
optimal_policy_vi, value_function_vi

"""**2. Policy Iteration algorrithm**"""

def policy_iteration(states, actions, P, base_rewards, ad_cost, gamma, epsilon):
    # Initialization:
    V = {s: 0.0 for s in states}
    policy = {s: 0 for s in states}  # Start with no ads for all states

    is_policy_stable = False
    while not is_policy_stable:
        # Policy Evaluation — loop until value function stabilizes
        while True:
            delta = 0
            for s in states:
                a = policy[s]
                reward = base_rewards[s] - (ad_cost if a == 1 else 0)
                expected_value = sum(P[a][s][s_prime - 1] * V[s_prime] for s_prime in states)
                new_v = reward + gamma * expected_value
                delta = max(delta, abs(V[s] - new_v))
                V[s] = new_v
            if delta < epsilon:
                break

        # Policy Improvement
        is_policy_stable = True
        for s in states:
            old_action = policy[s]
            action_values = []
            for a in actions:
                reward = base_rewards[s] - (ad_cost if a == 1 else 0)
                expected_value = sum(P[a][s][s_prime - 1] * V[s_prime] for s_prime in states)
                action_values.append(reward + gamma * expected_value)

            best_action = actions[action_values.index(max(action_values))]

            if old_action != best_action:
                is_policy_stable = False
                policy[s] = best_action

    return policy, V

# Run policy iteration
optimal_policy_pi, value_function_pi = policy_iteration(states, actions, P, base_rewards, ad_cost, gamma, epsilon)

# Structured Output Formatter
def print_policy_results(policy, value_function):
    state_names = {
        1: "State 0 (<10 customers)",
        2: "State 1 (10–20 customers)",
        3: "State 2 (>20 customers)"
    }

    action_names = {
        0: "Do NOT Advertise",
        1: "Advertise"
    }

    print("\nFinal Results from Policy Iteration")
    for state in sorted(state_names.keys()):
        print(f"{state_names[state]}")
        print(f" Value:  {value_function[state]:8.2f}")
        print(f"Best Action: {action_names[policy[state]]}")

# Display results
print_policy_results(optimal_policy_pi, value_function_pi)